{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff15c4a9",
   "metadata": {},
   "source": [
    "# Titanic ETL Pipeline\n",
    "\n",
    "**Objective:**\n",
    "- Create an automated pipeline for data ingestion, cleaning, transformation, and loading.\n",
    "- Utilize tools like Pandas and Scikit-learn for preprocessing and feature engineering.\n",
    "- Ensure reproducibility and scalability of the ETL process.\n",
    "\n",
    "**Dataset:** Titanic Dataset from Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151aeeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\saksh\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\saksh\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.15.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/10.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.7 MB 935.5 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/10.7 MB 935.5 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/10.7 MB 935.5 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/10.7 MB 935.5 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/10.7 MB 935.5 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/10.7 MB 935.5 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/10.7 MB 935.5 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/10.7 MB 935.5 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.5/10.7 MB 935.5 kB/s eta 0:00:11\n",
      "   -- ------------------------------------- 0.8/10.7 MB 259.7 kB/s eta 0:00:38\n",
      "   -- ------------------------------------- 0.8/10.7 MB 259.7 kB/s eta 0:00:38\n",
      "   -- ------------------------------------- 0.8/10.7 MB 259.7 kB/s eta 0:00:38\n",
      "   -- ------------------------------------- 0.8/10.7 MB 259.7 kB/s eta 0:00:38\n",
      "   -- ------------------------------------- 0.8/10.7 MB 259.7 kB/s eta 0:00:38\n",
      "   -- ------------------------------------- 0.8/10.7 MB 259.7 kB/s eta 0:00:38\n",
      "   --- ------------------------------------ 1.0/10.7 MB 242.4 kB/s eta 0:00:40\n",
      "   --- ------------------------------------ 1.0/10.7 MB 242.4 kB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 1.3/10.7 MB 289.9 kB/s eta 0:00:33\n",
      "   ---- ----------------------------------- 1.3/10.7 MB 289.9 kB/s eta 0:00:33\n",
      "   ----- ---------------------------------- 1.6/10.7 MB 311.7 kB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 1.6/10.7 MB 311.7 kB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 1.6/10.7 MB 311.7 kB/s eta 0:00:30\n",
      "   ------ --------------------------------- 1.8/10.7 MB 326.1 kB/s eta 0:00:28\n",
      "   ------ --------------------------------- 1.8/10.7 MB 326.1 kB/s eta 0:00:28\n",
      "   ------- -------------------------------- 2.1/10.7 MB 351.1 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 2.1/10.7 MB 351.1 kB/s eta 0:00:25\n",
      "   ------- -------------------------------- 2.1/10.7 MB 351.1 kB/s eta 0:00:25\n",
      "   -------- ------------------------------- 2.4/10.7 MB 365.1 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 2.4/10.7 MB 365.1 kB/s eta 0:00:23\n",
      "   -------- ------------------------------- 2.4/10.7 MB 365.1 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 2.6/10.7 MB 363.8 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 2.6/10.7 MB 363.8 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 2.6/10.7 MB 363.8 kB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 2.9/10.7 MB 368.1 kB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 3.1/10.7 MB 394.5 kB/s eta 0:00:20\n",
      "   ------------ --------------------------- 3.4/10.7 MB 414.7 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 3.4/10.7 MB 414.7 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 3.7/10.7 MB 429.6 kB/s eta 0:00:17\n",
      "   ------------- -------------------------- 3.7/10.7 MB 429.6 kB/s eta 0:00:17\n",
      "   -------------- ------------------------- 3.9/10.7 MB 439.0 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 3.9/10.7 MB 439.0 kB/s eta 0:00:16\n",
      "   --------------- ------------------------ 4.2/10.7 MB 448.5 kB/s eta 0:00:15\n",
      "   --------------- ------------------------ 4.2/10.7 MB 448.5 kB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 4.5/10.7 MB 457.2 kB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 4.7/10.7 MB 473.5 kB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 4.7/10.7 MB 473.5 kB/s eta 0:00:13\n",
      "   ------------------ --------------------- 5.0/10.7 MB 480.6 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 5.0/10.7 MB 480.6 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 5.0/10.7 MB 480.6 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 5.2/10.7 MB 479.6 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 5.2/10.7 MB 479.6 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 5.5/10.7 MB 487.6 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 5.8/10.7 MB 503.1 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 6.0/10.7 MB 511.2 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 6.0/10.7 MB 511.2 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 6.3/10.7 MB 513.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 6.3/10.7 MB 513.7 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 6.6/10.7 MB 518.0 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 6.8/10.7 MB 532.2 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 7.1/10.7 MB 545.9 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 7.1/10.7 MB 545.9 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 7.1/10.7 MB 545.9 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 7.3/10.7 MB 538.9 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 7.3/10.7 MB 538.9 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 7.6/10.7 MB 538.6 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 7.6/10.7 MB 538.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 7.9/10.7 MB 546.3 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 8.1/10.7 MB 551.9 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 8.4/10.7 MB 563.8 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 8.7/10.7 MB 573.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 8.9/10.7 MB 583.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 9.2/10.7 MB 593.8 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 9.4/10.7 MB 604.1 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 9.4/10.7 MB 604.1 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 9.7/10.7 MB 603.4 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 10.0/10.7 MB 612.3 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 10.2/10.7 MB 621.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.7 MB 621.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.7 MB 619.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.7 MB 619.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.7 MB 619.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  10.5/10.7 MB 619.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 601.7 kB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7ae3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f273159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "..           ...       ...     ...   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket     Fare Cabin Embarked  \n",
      "0        0         A/5 21171   7.2500   NaN        S  \n",
      "1        0          PC 17599  71.2833   C85        C  \n",
      "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3        0            113803  53.1000  C123        S  \n",
      "4        0            373450   8.0500   NaN        S  \n",
      "..     ...               ...      ...   ...      ...  \n",
      "886      0            211536  13.0000   NaN        S  \n",
      "887      0            112053  30.0000   B42        S  \n",
      "888      2        W./C. 6607  23.4500   NaN        S  \n",
      "889      0            111369  30.0000  C148        C  \n",
      "890      0            370376   7.7500   NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Ingestion\n",
    "train_df= pd.read_csv(\"C:\\\\Users\\\\saksh\\\\OneDrive\\\\Desktop\\\\Python Practice\\\\titanic\\\\train.csv\")\n",
    "\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46344b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_2676\\273282871.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Basic Data Cleaning\n",
    "train_df = train_df.drop(columns=[\"PassengerId\", \"Ticket\", \"Cabin\"])\n",
    "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Dr', 'Rev', 'Col', 'Mlle', 'Major', 'Ms', 'Mme', 'Don', 'Lady', 'Sir',\n",
      "       'Capt', 'the Countess', 'Jonkheer'],\n",
      "      dtype='object', name='Title')\n",
      "     PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch  \\\n",
      "0              1         0       3    male  22.0      1      0   \n",
      "1              2         1       1  female  38.0      1      0   \n",
      "2              3         1       3  female  26.0      0      0   \n",
      "3              4         1       1  female  35.0      1      0   \n",
      "4              5         0       3    male  35.0      0      0   \n",
      "..           ...       ...     ...     ...   ...    ...    ...   \n",
      "886          887         0       2    male  27.0      0      0   \n",
      "887          888         1       1  female  19.0      0      0   \n",
      "888          889         0       3  female   NaN      1      2   \n",
      "889          890         1       1    male  26.0      0      0   \n",
      "890          891         0       3    male  32.0      0      0   \n",
      "\n",
      "               Ticket     Fare Cabin Embarked Title  \n",
      "0           A/5 21171   7.2500   NaN        S    Mr  \n",
      "1            PC 17599  71.2833   C85        C   Mrs  \n",
      "2    STON/O2. 3101282   7.9250   NaN        S  Miss  \n",
      "3              113803  53.1000  C123        S   Mrs  \n",
      "4              373450   8.0500   NaN        S    Mr  \n",
      "..                ...      ...   ...      ...   ...  \n",
      "886            211536  13.0000   NaN        S  Rare  \n",
      "887            112053  30.0000   B42        S  Miss  \n",
      "888        W./C. 6607  23.4500   NaN        S  Miss  \n",
      "889            111369  30.0000  C148        C    Mr  \n",
      "890            370376   7.7500   NaN        Q    Mr  \n",
      "\n",
      "[891 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Extracting Title\n",
    "train_df= pd.read_csv(\"C:\\\\Users\\\\saksh\\\\OneDrive\\\\Desktop\\\\Python Practice\\\\titanic\\\\train.csv\")\n",
    "train_df['Title'] = train_df['Name'].str.extract(r',\\s*([^\\.]+)\\s*\\.')\n",
    "train_df = train_df.drop(columns=[\"Name\"])\n",
    "rare_titles = train_df['Title'].value_counts()[train_df['Title'].value_counts() < 10].index\n",
    "train_df['Title'] = train_df['Title'].replace(rare_titles, 'Rare')\n",
    "print(rare_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2410474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Creating FamilySize\n",
    "train_df= pd.read_csv(\"C:\\\\Users\\\\saksh\\\\OneDrive\\\\Desktop\\\\Python Practice\\\\titanic\\\\train.csv\")\n",
    "train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n",
    "train_df = train_df.drop(columns=[\"SibSp\", \"Parch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Encoding and Scaling\n",
    "train_df= pd.read_csv(\"C:\\\\Users\\\\saksh\\\\OneDrive\\\\Desktop\\\\Python Practice\\\\titanic\\\\train.csv\")\n",
    "numeric_features = ['Age', 'Fare', 'FamilySize']\n",
    "categorical_features = ['Sex', 'Embarked', 'Title']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "print(numeric_transformer)\n",
    "\n",
    "# Encoding categorical features manually with LabelEncoder\n",
    "for col in categorical_features:\n",
    "    train_df[col] = LabelEncoder().fit_transform(train_df[col])\n",
    "\n",
    "train_df[numeric_features] = numeric_transformer.fit_transform(train_df[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e793d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data (Optional)\n",
    "X = train_df.drop(\"Survived\", axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f1050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Pipeline complete. Cleaned data saved as 'titanic_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Saving cleaned dataset\n",
    "import pandas as pd\n",
    "cleaned_data = pd.concat([X, y], axis=1)\n",
    "cleaned_data.to_csv(\"C:\\\\Users\\\\saksh\\\\OneDrive\\\\Desktop\\\\Python Practice\\\\titanic\\\\train.csv_cleaned.csv\", index=False)\n",
    "print(\"ETL Pipeline complete. Cleaned data saved as 'titanic_cleaned.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025cce5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
